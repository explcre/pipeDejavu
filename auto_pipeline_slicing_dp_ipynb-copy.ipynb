{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/explcre/pipeDejavu/blob/main/auto_pipeline_slicing_dp_ipynb-copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a simple input tensor with 7 elements\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0], requires_grad=True)\n",
        "\n",
        "# Apply the max operation on the input tensor\n",
        "max_value, max_idx = torch.max(x,dim=0)\n",
        "\n",
        "# Define the loss function as the max value\n",
        "loss = max_value\n",
        "\n",
        "# Compute the gradients\n",
        "loss.backward()\n",
        "\n",
        "# Print the gradients with respect to the input tensor\n",
        "print(\"Gradients: \", x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C84lnwV3YjXK",
        "outputId": "0663090e-13bf-44d3-9cf2-39b9228c0420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients:  tensor([0., 0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def knapsack(items, capacity):\n",
        "    n = len(items)\n",
        "    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        weight, value = items[i - 1]\n",
        "        for w in range(capacity + 1):\n",
        "            if weight <= w:\n",
        "                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - weight] + value)\n",
        "            else:\n",
        "                dp[i][w] = dp[i - 1][w]\n",
        "\n",
        "    selected_items = []\n",
        "    i, w = n, capacity\n",
        "    while i > 0 and w > 0:\n",
        "        weight, value = items[i - 1]\n",
        "        if dp[i][w] != dp[i - 1][w]:\n",
        "            selected_items.append(i - 1)\n",
        "            w -= weight\n",
        "        i -= 1\n",
        "\n",
        "    return dp[n][capacity], selected_items\n",
        "\n",
        "# Example usage:\n",
        "items = [(2, 3), (3, 4), (4, 5), (5, 6)]  # (weight, value)\n",
        "capacity = 5\n",
        "start=time.time()\n",
        "max_value, selected_items = knapsack(items, capacity)\n",
        "end=time.time()\n",
        "print(max_value, selected_items)\n",
        "print(\"time=\",end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6vbkE50S5O",
        "outputId": "8e537c84-b50d-4cb1-9567-38371762cb50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 [1, 0]\n",
            "time= 0.00019359588623046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def soft_knapsack(items, capacity, iterations=5000, learning_rate=0.01):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.Adam([item_selection], lr=learning_rate)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    final_selection = torch.sigmoid(item_selection) > 0.5\n",
        "    selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "    max_value = torch.sum(final_selection * values).item()\n",
        "\n",
        "    return max_value, selected_items\n",
        "\n",
        "# Example usage:\n",
        "items = [(2, 3), (3, 4), (4, 5), (5, 6)]  # (weight, value)\n",
        "capacity = 5\n",
        "start=time.time()\n",
        "max_value, selected_items = soft_knapsack(items, capacity)\n",
        "end=time.time()\n",
        "print(max_value, selected_items)\n",
        "print(\"time=\",end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-onagjca0Wnd",
        "outputId": "55642e5d-7314-437c-8b79-c0621d5adfdb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0 [0, 1]\n",
            "time= 3.351736068725586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_knapsack(items, capacity, iterations=5000, learning_rate=0.01, temperature=0.5):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "    selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "    max_value = torch.sum(final_selection * values).item()\n",
        "\n",
        "    return max_value, selected_items"
      ],
      "metadata": {
        "id": "7pxjDrjH6CW0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_knapsack(items, capacity, iterations=5000, learning_rate=0.01, temperature=0.5):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Print intermediate results\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\n",
        "\n",
        "    final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "    selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "    max_value = torch.sum(final_selection * values).item()\n",
        "\n",
        "    return max_value, selected_items"
      ],
      "metadata": {
        "id": "Ef3fBuLy6fQ5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\"])\n",
        "\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Update best solutions\n",
        "        final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "        selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "        max_value = torch.sum(final_selection * values).item()\n",
        "        current_solution = Solution(max_value, selected_items, final_selection)\n",
        "\n",
        "        if len(best_solutions) < n_best:\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "        elif max_value > best_solutions[-1].value:\n",
        "            best_solutions.pop()\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "\n",
        "    return best_solutions"
      ],
      "metadata": {
        "id": "QtR1gr6_7jFT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\"])\n",
        "\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Update best solutions\n",
        "        final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "        selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "        max_value = torch.sum(final_selection * values).item()\n",
        "        current_solution = Solution(max_value, selected_items, final_selection)\n",
        "\n",
        "        if len(best_solutions) < n_best:\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "        elif max_value > best_solutions[-1].value:\n",
        "            best_solutions.pop()\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "\n",
        "        # Print intermediate results\n",
        "        if (i + 1) % print_every == 0:\n",
        "            best_solution = best_solutions[0]\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\n",
        "            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\n",
        "\n",
        "    max_value = best_solutions[0].value\n",
        "    selected_items = best_solutions[0].items\n",
        "    return max_value, selected_items, best_solutions\n",
        "\n",
        "# Time and run the differentiable knapsack solution\n",
        "start_time = time.time()\n",
        "max_value_diff, selected_items_diff, best_solutions = soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Differentiable Knapsack: max_value={max_value_diff}, selected_items={selected_items_diff}, time={end_time - start_time:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvVbbex48qz9",
        "outputId": "b972635f-c478-4e86-9fdd-3ec5d77df1ab"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500: loss=56169.26, total_value=1222.51, total_weight=1239.57\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 1000: loss=-1010.93, total_value=1011.20, total_weight=1000.52\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 1500: loss=-1957.79, total_value=1958.61, total_weight=1000.90\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 2000: loss=-4612.74, total_value=4612.74, total_weight=996.17\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 2500: loss=-5562.35, total_value=5567.02, total_weight=1002.16\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 3000: loss=-5711.57, total_value=5716.15, total_weight=1002.14\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 3500: loss=-5733.84, total_value=5737.21, total_weight=1001.84\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 4000: loss=-5737.65, total_value=5740.53, total_weight=1001.70\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 4500: loss=-5738.83, total_value=5741.48, total_weight=1001.63\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Iteration 5000: loss=-5739.24, total_value=5741.77, total_weight=1001.59\n",
            "Best solution so far: value=22315.0, items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499]\n",
            "Differentiable Knapsack: max_value=22315.0, selected_items=[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499], time=8.15s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\"])\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        loss = -(total_value - capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Update best solutions only if the solution is valid (total_weight <= capacity)\n",
        "        if total_weight <= capacity:\n",
        "            final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "            selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "            max_value = torch.sum(final_selection * values).item()\n",
        "            current_solution = Solution(max_value, selected_items, final_selection)\n",
        "\n",
        "            if len(best_solutions) < n_best:\n",
        "                best_solutions.append(current_solution)\n",
        "                best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "            elif max_value > best_solutions[-1].value:\n",
        "                best_solutions.pop()\n",
        "                best_solutions.append(current_solution)\n",
        "                best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "\n",
        "        # Print intermediate results\n",
        "        if (i + 1) % print_every == 0:\n",
        "            best_solution = best_solutions[0]\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\n",
        "            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\n",
        "\n",
        "    max_value = best_solutions[0].value\n",
        "    selected_items = best_solutions[0].items\n",
        "    return max_value, selected_items, best_solutions\n"
      ],
      "metadata": {
        "id": "U7pW_XWH-8CS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\"])\n",
        "\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500, penalty_factor=1e7):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        # Multiply capacity_penalty by a large constant\n",
        "        loss = -(total_value - penalty_factor * capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Update best solutions\n",
        "        final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "        selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "        max_value = torch.sum(final_selection * values).item()\n",
        "        current_solution = Solution(max_value, selected_items, final_selection)\n",
        "\n",
        "        if len(best_solutions) < n_best:\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "        elif max_value > best_solutions[-1].value:\n",
        "            best_solutions.pop()\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "\n",
        "        # Print intermediate results\n",
        "        if (i + 1) % print_every == 0:\n",
        "            best_solution = best_solutions[0]\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\n",
        "            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\n",
        "\n",
        "    max_value = best_solutions[0].value\n",
        "    selected_items = best_solutions[0].items\n",
        "    return max_value, selected_items, best_solutions\n"
      ],
      "metadata": {
        "id": "CT2ePOF6ARNO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knapsack(items, capacity, print_every=500):\n",
        "    n = len(items)\n",
        "    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    best_solutions_dp = []\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        weight, value = items[i - 1]\n",
        "        for w in range(capacity + 1):\n",
        "            if weight <= w:\n",
        "                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - weight] + value)\n",
        "            else:\n",
        "                dp[i][w] = dp[i - 1][w]\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            best_solutions_dp.append(dp[i][-1])\n",
        "            print(f\"Iteration {i}: Best solution so far for DP: value={dp[i][-1]}\")\n",
        "\n",
        "    selected_items = []\n",
        "    i, w = n, capacity\n",
        "    while i > 0 and w > 0:\n",
        "        weight, value = items[i - 1]\n",
        "        if dp[i][w] != dp[i - 1][w]:\n",
        "            selected_items.append(i - 1)\n",
        "            w -= weight\n",
        "        i -= 1\n",
        "\n",
        "    return dp[n][capacity], selected_items, best_solutions_dp\n"
      ],
      "metadata": {
        "id": "FTZ754y2SdXU"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def knapsack(items, capacity, print_every=500,timer_every=10):\n",
        "    n = len(items)\n",
        "    dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    best_solutions_dp = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in range(1, n + 1):\n",
        "        weight, value = items[i - 1]\n",
        "        for w in range(capacity + 1):\n",
        "            if weight <= w:\n",
        "                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - weight] + value)\n",
        "            else:\n",
        "                dp[i][w] = dp[i - 1][w]\n",
        "            \n",
        "        if i % timer_every == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            best_solutions_dp.append((elapsed_time, dp[i][-1]))\n",
        "        if i % print_every == 0:\n",
        "            \n",
        "            print(f\"Iteration {i}: Best solution so far for DP: value={dp[i][-1]}, elapsed_time={elapsed_time:.2f}s\")\n",
        "\n",
        "    selected_items = []\n",
        "    i, w = n, capacity\n",
        "    while i > 0 and w > 0:\n",
        "        weight, value = items[i - 1]\n",
        "        if dp[i][w] != dp[i - 1][w]:\n",
        "            selected_items.append(i - 1)\n",
        "            w -= weight\n",
        "        i -= 1\n",
        "\n",
        "    return dp[n][capacity], selected_items, best_solutions_dp\n"
      ],
      "metadata": {
        "id": "nXTFNToSWbKr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "#Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\"])\n",
        "\n",
        "Solution = namedtuple(\"Solution\", [\"value\", \"items\", \"selection\", \"elapsed_time\"])\n",
        "\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=10000, learning_rate=100, temperature=0.5, print_every=500, penalty_factor=1e7):\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        # Multiply capacity_penalty by a large constant\n",
        "        loss = -(total_value - penalty_factor * capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "        if total_weight <= capacity:\n",
        "          # Update best solutions\n",
        "          final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "          selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "          max_value = torch.sum(final_selection * values).item()\n",
        "          current_solution = Solution(max_value, selected_items, final_selection,time.time() - start_time)\n",
        "\n",
        "          if len(best_solutions) < n_best:\n",
        "              best_solutions.append(current_solution)\n",
        "              best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "          elif max_value > best_solutions[-1].value:\n",
        "              best_solutions.pop()\n",
        "              best_solutions.append(current_solution)\n",
        "              best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "              # Print intermediate results\n",
        "        if (i + 1) % print_every == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            best_solution = best_solutions[0]\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}, elapsed_time={elapsed_time:.2f}s\")\n",
        "            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\n",
        "\n",
        "    max_value = best_solutions[0].value\n",
        "    selected_items = best_solutions[0].items\n",
        "\n",
        "    best_solutions_diff = sorted([(s.elapsed_time, s.value) for s in best_solutions], key=lambda x: x[0])\n",
        "    #best_solutions_diff = [(s.elapsed_time, s.value) for s in best_solutions]\n",
        "\n",
        "    return max_value, selected_items, best_solutions_diff\n",
        "\n",
        "\n",
        "'''\n",
        "def soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500, penalty_factor=1e7):\n",
        "    start_time = time.time()\n",
        "    n = len(items)\n",
        "    weights, values = zip(*items)\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "    item_selection = torch.rand(n, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\n",
        "\n",
        "    best_solutions = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "        soft_selection = torch.sigmoid(item_selection / temperature)\n",
        "        total_weight = torch.sum(soft_selection * weights)\n",
        "        total_value = torch.sum(soft_selection * values)\n",
        "        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\n",
        "\n",
        "        # Multiply capacity_penalty by a large constant\n",
        "        loss = -(total_value - penalty_factor * capacity_penalty)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clamp item_selection values between -5 and 5\n",
        "        item_selection.data.clamp_(-5, 5)\n",
        "\n",
        "        # Update best solutions\n",
        "        final_selection = torch.sigmoid(item_selection / temperature) > 0.5\n",
        "        selected_items = [i for i, selected in enumerate(final_selection) if selected]\n",
        "        max_value = torch.sum(final_selection * values).item()\n",
        "        current_solution = Solution(max_value, selected_items, final_selection, time.time() - start_time)\n",
        "\n",
        "        if len(best_solutions) < n_best:\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "        elif max_value > best_solutions[-1].value:\n",
        "            best_solutions.pop()\n",
        "            best_solutions.append(current_solution)\n",
        "            best_solutions.sort(key=lambda x: x.value, reverse=True)\n",
        "\n",
        "        # Print intermediate results\n",
        "        if (i + 1) % print_every == 0:\n",
        "            best_solution = best_solutions[0]\n",
        "            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\n",
        "            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\n",
        "\n",
        "    max_value = best_solutions[0].value\n",
        "    selected_items = best_solutions[0].items\n",
        "    best_solutions_diff = [(s.elapsed_time, s.value) for s in best_solutions]\n",
        "\n",
        "    return max_value, selected_items, best_solutions_diff\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "lXGejWTBWnEr",
        "outputId": "dfad7ad2-0c84-4d93-da2f-19399b07a450"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef soft_knapsack(items, capacity, n_best=5, iterations=5000, learning_rate=0.01, temperature=0.5, print_every=500, penalty_factor=1e7):\\n    start_time = time.time()\\n    n = len(items)\\n    weights, values = zip(*items)\\n    weights = torch.tensor(weights, dtype=torch.float)\\n    values = torch.tensor(values, dtype=torch.float)\\n\\n    item_selection = torch.rand(n, requires_grad=True)\\n    optimizer = optim.RMSprop([item_selection], lr=learning_rate)\\n\\n    best_solutions = []\\n\\n    for i in range(iterations):\\n        optimizer.zero_grad()\\n        soft_selection = torch.sigmoid(item_selection / temperature)\\n        total_weight = torch.sum(soft_selection * weights)\\n        total_value = torch.sum(soft_selection * values)\\n        capacity_penalty = torch.clamp(total_weight - capacity, min=0) ** 2\\n\\n        # Multiply capacity_penalty by a large constant\\n        loss = -(total_value - penalty_factor * capacity_penalty)\\n        loss.backward()\\n        optimizer.step()\\n\\n        # Clamp item_selection values between -5 and 5\\n        item_selection.data.clamp_(-5, 5)\\n\\n        # Update best solutions\\n        final_selection = torch.sigmoid(item_selection / temperature) > 0.5\\n        selected_items = [i for i, selected in enumerate(final_selection) if selected]\\n        max_value = torch.sum(final_selection * values).item()\\n        current_solution = Solution(max_value, selected_items, final_selection, time.time() - start_time)\\n\\n        if len(best_solutions) < n_best:\\n            best_solutions.append(current_solution)\\n            best_solutions.sort(key=lambda x: x.value, reverse=True)\\n        elif max_value > best_solutions[-1].value:\\n            best_solutions.pop()\\n            best_solutions.append(current_solution)\\n            best_solutions.sort(key=lambda x: x.value, reverse=True)\\n\\n        # Print intermediate results\\n        if (i + 1) % print_every == 0:\\n            best_solution = best_solutions[0]\\n            print(f\"Iteration {i+1}: loss={loss.item():.2f}, total_value={total_value.item():.2f}, total_weight={total_weight.item():.2f}\")\\n            print(f\"Best solution so far: value={best_solution.value}, items={best_solution.items}\")\\n\\n    max_value = best_solutions[0].value\\n    selected_items = best_solutions[0].items\\n    best_solutions_diff = [(s.elapsed_time, s.value) for s in best_solutions]\\n\\n    return max_value, selected_items, best_solutions_diff\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHXnHoDAWnFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def generate_test_case(n, weight_range, value_range, capacity):\n",
        "    items = [(random.randint(*weight_range), random.randint(*value_range)) for _ in range(n)]\n",
        "    return items, capacity"
      ],
      "metadata": {
        "id": "vuN3YriTSpAl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate a large test case with a large capacity\n",
        "n = 1000\n",
        "weight_range = (1, 1000)\n",
        "value_range = (1, 100)\n",
        "capacity = 100000\n",
        "\n",
        "# Test and visualization\n",
        "items, capacity = generate_test_case(n, weight_range, value_range, capacity)\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_dp, selected_items_dp, best_solutions_dp = knapsack(items, capacity)\n",
        "end_time = time.time()\n",
        "elapsed_time_dp = end_time - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_diff, selected_items_diff, best_solutions_diff = soft_knapsack(items, capacity,iterations=10000,learning_rate=100)\n",
        "end_time = time.time()\n",
        "elapsed_time_diff = end_time - start_time\n",
        "\n",
        "\n",
        "# Plot the best searched solution so far vs elapsed time for both methods\n",
        "dp_time, dp_values = zip(*best_solutions_dp)\n",
        "diff_time, diff_values = zip(*best_solutions_diff)\n",
        "\n",
        "plt.plot(dp_time, dp_values, label=\"Dynamic Programming\")\n",
        "plt.plot(diff_time, diff_values, label=\"Differentiable Knapsack\")\n",
        "plt.xlabel(\"Elapsed Time (s)\")\n",
        "plt.ylabel(\"Best Searched Solution So Far\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QgQh5xb4Xedb",
        "outputId": "e63beadd-7c3b-49f6-ad46-c4db0982d63b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500: Best solution so far for DP: value=18080, elapsed_time=21.32s\n",
            "Iteration 1000: Best solution so far for DP: value=25685, elapsed_time=43.75s\n",
            "Iteration 500: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=1.43s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 1000: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=3.04s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 1500: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=5.43s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 2000: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=6.75s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 2500: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=8.22s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 3000: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=9.55s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 3500: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=10.96s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 4000: loss=-2.26, total_value=2.26, total_weight=23.19, elapsed_time=12.27s\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 4500: loss=-119.39, total_value=119.39, total_weight=29.47, elapsed_time=13.71s\n",
            "Best solution so far: value=117.0, items=[35, 82, 903]\n",
            "Iteration 5000: loss=-4960.18, total_value=4960.18, total_weight=4682.19, elapsed_time=15.03s\n",
            "Best solution so far: value=4956.0, items=[35, 42, 44, 64, 82, 112, 113, 127, 141, 155, 170, 188, 189, 205, 208, 215, 218, 258, 259, 260, 277, 286, 304, 319, 365, 367, 402, 403, 428, 441, 452, 478, 486, 492, 523, 529, 563, 564, 576, 624, 639, 643, 655, 665, 666, 673, 676, 694, 697, 703, 713, 714, 718, 777, 798, 828, 829, 837, 848, 850, 869, 875, 891, 903, 938, 942, 969, 975, 976, 993, 998]\n",
            "Iteration 5500: loss=-2.26, total_value=2.26, total_weight=23.17, elapsed_time=17.29s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 6000: loss=-119.41, total_value=119.41, total_weight=29.40, elapsed_time=18.96s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 6500: loss=-6002.47, total_value=6002.47, total_weight=6048.18, elapsed_time=20.38s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 7000: loss=-23679.31, total_value=23679.31, total_weight=90650.95, elapsed_time=21.72s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 7500: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=23.16s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 8000: loss=-2.25, total_value=2.25, total_weight=23.17, elapsed_time=24.47s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 8500: loss=-87.29, total_value=87.29, total_weight=26.27, elapsed_time=25.91s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 9000: loss=-2142.56, total_value=2142.56, total_weight=939.10, elapsed_time=27.24s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 9500: loss=-24534.46, total_value=24534.46, total_weight=93596.86, elapsed_time=29.06s\n",
            "Best solution so far: value=24785.0, items=[3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 67, 68, 79, 82, 84, 90, 92, 101, 104, 106, 107, 112, 113, 115, 118, 119, 122, 125, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 153, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 194, 203, 204, 205, 207, 208, 210, 213, 215, 218, 222, 226, 228, 231, 234, 239, 240, 241, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 275, 277, 278, 282, 283, 285, 286, 287, 297, 298, 299, 304, 305, 310, 312, 314, 317, 318, 319, 322, 323, 325, 330, 333, 336, 338, 340, 341, 342, 343, 350, 351, 352, 365, 367, 371, 378, 380, 381, 388, 390, 391, 392, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 426, 428, 429, 432, 434, 435, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 467, 471, 477, 478, 479, 482, 483, 486, 487, 490, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 532, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 590, 592, 593, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 635, 636, 639, 640, 641, 642, 643, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 675, 676, 680, 682, 688, 689, 690, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 722, 723, 724, 726, 733, 746, 748, 751, 755, 756, 760, 763, 765, 767, 770, 773, 774, 776, 777, 786, 787, 788, 789, 791, 795, 796, 797, 798, 807, 811, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 856, 859, 868, 869, 871, 874, 875, 881, 882, 888, 891, 893, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 926, 927, 928, 930, 933, 936, 937, 938, 941, 942, 947, 952, 953, 958, 959, 960, 963, 964, 965, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 993, 997, 998]\n",
            "Iteration 10000: loss=-2.25, total_value=2.25, total_weight=23.16, elapsed_time=31.28s\n",
            "Best solution so far: value=25353.0, items=[1, 3, 4, 6, 7, 9, 10, 14, 15, 20, 21, 26, 27, 30, 31, 33, 35, 39, 41, 42, 43, 44, 47, 48, 53, 57, 62, 64, 66, 68, 79, 82, 84, 87, 90, 92, 95, 101, 104, 105, 107, 112, 113, 115, 118, 119, 122, 125, 126, 127, 132, 135, 136, 137, 138, 139, 140, 141, 142, 144, 148, 155, 158, 168, 170, 171, 172, 173, 181, 182, 183, 187, 188, 189, 190, 192, 200, 203, 204, 205, 207, 208, 209, 210, 213, 215, 218, 222, 226, 228, 231, 237, 239, 240, 244, 246, 247, 248, 250, 258, 259, 260, 263, 264, 271, 273, 275, 277, 278, 283, 285, 286, 287, 297, 299, 304, 305, 310, 312, 314, 317, 318, 319, 323, 330, 333, 336, 338, 341, 342, 343, 350, 351, 352, 365, 366, 367, 369, 371, 378, 380, 381, 388, 390, 391, 392, 394, 396, 402, 403, 405, 416, 417, 419, 420, 421, 423, 427, 428, 429, 432, 434, 435, 437, 441, 442, 448, 449, 451, 452, 456, 457, 458, 460, 464, 467, 471, 477, 478, 479, 486, 487, 490, 491, 492, 497, 503, 504, 505, 506, 510, 511, 513, 522, 523, 525, 528, 529, 530, 531, 533, 535, 538, 539, 541, 543, 561, 562, 563, 564, 565, 570, 574, 576, 579, 581, 582, 583, 592, 593, 594, 604, 610, 611, 612, 613, 615, 617, 618, 623, 624, 628, 633, 636, 639, 640, 641, 642, 643, 649, 650, 653, 655, 659, 660, 662, 665, 666, 667, 670, 673, 674, 675, 676, 682, 688, 694, 696, 697, 698, 699, 702, 703, 706, 709, 712, 713, 714, 717, 718, 721, 722, 723, 724, 726, 733, 746, 748, 751, 756, 760, 763, 765, 767, 770, 774, 776, 777, 786, 787, 788, 789, 790, 791, 795, 796, 797, 798, 806, 807, 811, 815, 818, 828, 829, 830, 832, 834, 836, 837, 840, 841, 843, 845, 847, 848, 850, 854, 856, 859, 862, 864, 868, 869, 871, 874, 875, 881, 882, 886, 888, 891, 893, 894, 898, 903, 907, 908, 909, 913, 916, 919, 920, 921, 927, 928, 930, 933, 934, 937, 938, 942, 947, 952, 953, 956, 958, 959, 960, 963, 964, 969, 971, 973, 974, 975, 976, 982, 985, 986, 987, 990, 991, 993, 997, 998]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate a large test case with a large capacity\n",
        "n = 1000\n",
        "weight_range = (1, 1000)\n",
        "value_range = (1, 100)\n",
        "capacity = 100000\n",
        "\n",
        "# Test and visualization\n",
        "items, capacity = generate_test_case(n, weight_range, value_range, capacity)\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_dp, selected_items_dp, best_solutions_dp = knapsack(items, capacity)\n",
        "end_time = time.time()\n",
        "elapsed_time_dp = end_time - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_diff, selected_items_diff, best_solutions_diff = soft_knapsack(items, capacity, iterations=10000, learning_rate=100)\n",
        "end_time = time.time()\n",
        "elapsed_time_diff = end_time - start_time\n",
        "\n",
        "# Prepare data for visualization\n",
        "x_dp = np.linspace(0, elapsed_time_dp, len(best_solutions_dp))\n",
        "x_diff = np.linspace(0, elapsed_time_diff, len(best_solutions_diff))\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(x_dp, best_solutions_dp, label=\"Dynamic Programming\")\n",
        "plt.plot(x_diff, [sol.value for sol in best_solutions_diff], label=\"Differentiable Search\")\n",
        "plt.xlabel(\"Elapsed Time (s)\")\n",
        "plt.ylabel(\"Best Searched Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "GGJmgfN2ShdM",
        "outputId": "51884789-99d0-4723-cd40-1abf4e3e6dd5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-a345c8f7a323>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmax_value_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_items_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_solutions_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknapsack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0melapsed_time_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-c0deeee75ba8>\u001b[0m in \u001b[0;36mknapsack\u001b[0;34m(items, capacity, print_every)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapacity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "n = 5000\n",
        "weight_range = (1, 100)\n",
        "value_range = (1, 100)\n",
        "capacity = 1000\n",
        "'''\n",
        "\n",
        "\n",
        "# Generate a large test case with a large capacity\n",
        "n = 1000\n",
        "weight_range = (1, 1000)\n",
        "value_range = (1, 100)\n",
        "capacity = 100000\n",
        "items, capacity = generate_test_case(n, weight_range, value_range, capacity)\n",
        "\n",
        "\n",
        "\n",
        "# Save the test case to a file\n",
        "with open(\"test_case_large.txt\", \"w\") as f:\n",
        "    f.write(f\"{n} {capacity}\\n\")\n",
        "    for value, weight in zip(values, weights):\n",
        "        f.write(f\"{value} {weight}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_dp, selected_items_dp = knapsack(items, capacity)\n",
        "end_time = time.time()\n",
        "\n",
        "total_weight_dp = sum(items[i][0] for i in selected_items_dp)\n",
        "print(f\"Dynamic Programming: max_value={max_value_dp}, selected items: {selected_items_dp}, total_weight={total_weight_dp}, length of selected items={len(selected_items_dp)}, time={end_time - start_time:.2f}s\")\n",
        "\n",
        "start_time = time.time()\n",
        "max_value_diff, selected_items_diff, best_solutions = soft_knapsack(items, capacity, iterations=10000, learning_rate=100)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, best_solution in enumerate(best_solutions):\n",
        "    total_weight_best = sum(items[i][0] for i in best_solution.items)\n",
        "    print(f\"{i}-th Best solution: value={best_solution.value}, items={best_solution.items}, total_weight={total_weight_best}, length of selected items={len(best_solution.items)},\")\n",
        "\n",
        "total_weight_diff = sum(items[i][0] for i in selected_items_diff)\n",
        "print(f\"Differentiable Knapsack: max_value={max_value_diff}, selected items: {selected_items_diff}, total_weight={total_weight_diff}, length of selected items={len(selected_items_diff)}, time={end_time - start_time:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vSL4BKh4rZR",
        "outputId": "fb4e591a-a49b-4e8f-aefd-84b2387d17fa"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Programming: max_value=26491, selected items: [999, 998, 990, 981, 977, 974, 970, 966, 965, 963, 960, 958, 957, 955, 954, 949, 946, 945, 943, 942, 941, 939, 937, 929, 927, 926, 925, 923, 922, 919, 918, 917, 916, 913, 912, 911, 910, 908, 904, 902, 899, 898, 892, 891, 886, 877, 876, 875, 874, 873, 871, 870, 867, 866, 864, 863, 860, 858, 855, 854, 852, 851, 848, 846, 843, 842, 840, 839, 838, 837, 835, 830, 828, 826, 824, 823, 822, 816, 809, 801, 798, 797, 796, 791, 789, 787, 784, 783, 778, 777, 769, 768, 763, 762, 761, 754, 751, 749, 748, 747, 743, 738, 737, 736, 734, 730, 727, 725, 724, 722, 719, 717, 714, 711, 709, 708, 706, 704, 703, 695, 694, 693, 689, 688, 687, 681, 680, 669, 666, 665, 663, 662, 661, 660, 659, 656, 653, 652, 650, 649, 647, 645, 644, 643, 640, 638, 636, 635, 634, 633, 629, 628, 622, 618, 616, 615, 613, 610, 606, 605, 604, 603, 601, 599, 597, 594, 590, 589, 587, 585, 583, 580, 579, 572, 570, 567, 566, 564, 563, 562, 560, 557, 554, 543, 539, 537, 534, 533, 531, 529, 528, 524, 523, 520, 516, 513, 511, 509, 506, 502, 500, 498, 497, 494, 486, 484, 482, 481, 480, 479, 478, 477, 473, 470, 468, 462, 461, 460, 455, 453, 452, 450, 449, 445, 443, 441, 439, 438, 435, 431, 428, 426, 423, 421, 419, 418, 406, 402, 400, 399, 398, 391, 390, 385, 384, 382, 377, 374, 373, 371, 370, 368, 366, 365, 364, 363, 362, 361, 359, 357, 352, 351, 348, 345, 344, 343, 341, 339, 335, 332, 329, 328, 326, 325, 323, 318, 316, 312, 310, 307, 304, 300, 297, 296, 294, 293, 289, 288, 287, 283, 282, 281, 280, 276, 273, 264, 261, 260, 259, 258, 252, 250, 248, 245, 241, 240, 237, 234, 230, 229, 228, 226, 224, 222, 220, 219, 218, 211, 206, 200, 194, 193, 190, 187, 186, 185, 183, 175, 174, 172, 162, 160, 157, 156, 155, 154, 152, 150, 147, 144, 140, 138, 136, 135, 132, 131, 129, 128, 120, 118, 114, 113, 109, 104, 102, 101, 100, 96, 94, 92, 89, 87, 86, 85, 82, 81, 77, 72, 69, 68, 65, 63, 61, 59, 56, 52, 46, 42, 37, 36, 35, 33, 32, 30, 27, 23, 20, 16, 15, 12, 11, 10, 8, 7, 4], total_weight=99994, length of selected items=395, time=51.15s\n",
            "Iteration 500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 1000: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 1500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 2000: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 2500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 3000: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 3500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=0.0, items=[]\n",
            "Iteration 4000: loss=-95.35, total_value=95.35, total_weight=23.71\n",
            "Best solution so far: value=93.0, items=[15]\n",
            "Iteration 4500: loss=-499.48, total_value=499.48, total_weight=55.00\n",
            "Best solution so far: value=497.0, items=[15, 316, 384, 597, 687, 816, 923]\n",
            "Iteration 5000: loss=-6105.84, total_value=6105.84, total_weight=5434.46\n",
            "Best solution so far: value=6102.0, items=[15, 16, 20, 23, 30, 102, 109, 113, 118, 131, 135, 138, 140, 144, 160, 174, 219, 220, 222, 230, 260, 293, 307, 316, 318, 326, 343, 351, 357, 368, 384, 385, 390, 400, 418, 439, 450, 461, 473, 484, 498, 502, 513, 516, 557, 563, 566, 567, 589, 590, 597, 634, 650, 656, 660, 662, 669, 687, 708, 722, 747, 754, 761, 777, 784, 787, 816, 824, 828, 839, 843, 846, 851, 852, 854, 863, 899, 902, 904, 917, 923, 937, 939, 945, 946, 955, 965, 977, 990]\n",
            "Iteration 5500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 6000: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 6500: loss=-2.35, total_value=2.35, total_weight=22.69\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 7000: loss=-388.39, total_value=388.39, total_weight=37.82\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 7500: loss=-3778.56, total_value=3778.56, total_weight=1906.60\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 8000: loss=-23281.26, total_value=23281.26, total_weight=83679.91\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 8500: loss=-2.33, total_value=2.33, total_weight=22.68\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 9000: loss=-2.34, total_value=2.34, total_weight=22.69\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 9500: loss=-186.43, total_value=186.43, total_weight=24.76\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "Iteration 10000: loss=-2219.91, total_value=2219.91, total_weight=610.95\n",
            "Best solution so far: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999]\n",
            "0-th Best solution: value=25575.0, items=[4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=101203, length of selected items=385,\n",
            "1-th Best solution: value=25431.0, items=[4, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=99718, length of selected items=382,\n",
            "2-th Best solution: value=25431.0, items=[4, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=99718, length of selected items=382,\n",
            "3-th Best solution: value=25350.0, items=[4, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=99314, length of selected items=381,\n",
            "4-th Best solution: value=25283.0, items=[4, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=98917, length of selected items=380,\n",
            "Differentiable Knapsack: max_value=25575.0, selected items: [4, 5, 6, 7, 8, 10, 11, 12, 15, 16, 20, 23, 30, 32, 33, 35, 36, 37, 42, 48, 52, 56, 57, 59, 61, 63, 65, 68, 69, 72, 81, 82, 85, 86, 87, 92, 94, 96, 102, 104, 109, 113, 114, 118, 125, 127, 129, 131, 134, 135, 136, 138, 140, 144, 147, 152, 153, 155, 156, 158, 160, 162, 174, 183, 185, 186, 187, 190, 191, 193, 194, 200, 206, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 241, 248, 251, 252, 259, 260, 261, 264, 273, 275, 276, 281, 282, 283, 287, 288, 289, 291, 293, 294, 296, 297, 300, 304, 307, 309, 310, 312, 316, 318, 325, 326, 328, 329, 332, 335, 339, 341, 343, 344, 347, 348, 351, 357, 359, 361, 363, 364, 365, 368, 369, 370, 371, 373, 379, 382, 384, 385, 390, 391, 397, 398, 399, 400, 402, 406, 408, 418, 419, 421, 423, 426, 428, 435, 438, 439, 443, 445, 450, 453, 455, 460, 461, 462, 466, 468, 470, 471, 473, 477, 478, 479, 480, 481, 482, 484, 485, 486, 490, 494, 497, 498, 500, 502, 511, 513, 516, 520, 521, 522, 523, 524, 526, 528, 529, 533, 534, 537, 539, 543, 554, 556, 557, 560, 562, 563, 564, 566, 567, 570, 571, 572, 580, 583, 585, 586, 589, 590, 594, 597, 599, 601, 602, 603, 604, 606, 610, 613, 616, 618, 622, 628, 629, 633, 634, 636, 643, 645, 646, 647, 649, 650, 652, 653, 656, 659, 660, 662, 665, 666, 669, 680, 681, 686, 687, 688, 689, 693, 694, 695, 703, 704, 706, 708, 709, 711, 712, 714, 717, 722, 724, 725, 727, 734, 736, 738, 742, 743, 747, 748, 749, 751, 752, 754, 761, 762, 763, 768, 774, 775, 777, 778, 783, 784, 787, 789, 791, 796, 797, 798, 801, 809, 810, 812, 816, 817, 822, 823, 824, 826, 828, 830, 831, 835, 838, 839, 840, 841, 842, 843, 845, 846, 851, 852, 854, 855, 858, 860, 863, 864, 867, 868, 870, 871, 874, 875, 876, 886, 891, 892, 898, 899, 902, 904, 908, 911, 912, 913, 917, 918, 922, 923, 925, 926, 927, 929, 937, 939, 941, 942, 945, 946, 947, 949, 953, 954, 955, 957, 958, 960, 963, 965, 966, 970, 974, 977, 978, 981, 990, 993, 998, 999], total_weight=101203, length of selected items=385, time=30.86s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsjpejO9d2am",
        "outputId": "1a3ca3fa-2dc4-4850-d3b0-d4030caaa7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.10/dist-packages (from numba) (1.22.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzzwEIlId5zy",
        "outputId": "8221a8d3-864d-45f9-b931-6c0c63301799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cupy\n",
            "  Using cached cupy-12.0.0.tar.gz (2.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy) (1.22.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy) (0.8.1)\n",
            "Building wheels for collected packages: cupy\n",
            "  Building wheel for cupy (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 241, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 463, in run\n",
            "    _, build_failures = build(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 347, in build\n",
            "    wheel_file = _build_one(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 221, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 268, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 214, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 197, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
            "    self.stream.flush()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MdvGw37LQRH"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "#import cupy as cp\n",
        "from numba import jit, prange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xbpjSBHVQ3t",
        "outputId": "8ad28114-ab63-4fe9-8bde-fc77dcafa0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  7 03:19:53 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbCfjNagLQRK"
      },
      "outputs": [],
      "source": [
        "# simplification\n",
        "def f(L, maxl, cost, k, B):\n",
        "    if k == 1:\n",
        "        return ([L], B*max(0, L-maxl))\n",
        "    if k == L:\n",
        "        cost_ = max(1, maxl) * B\n",
        "        for i in range(k-1):\n",
        "         #   cost_ += cost[i][i]\n",
        "            cost_ += cost[i]\n",
        "        return ([1] * L, cost_)\n",
        "    \n",
        "    cost_best = float(\"inf\")\n",
        "    S_best = []\n",
        "    for i in reversed(range(k, L)):\n",
        "        S, cost_ = f(i, max(L-i, maxl), cost, k-1, B)\n",
        "        cost_ += max(0, L-i-maxl)*B\n",
        "        cost_ += cost[i-1]\n",
        "        if cost_ < cost_best:\n",
        "            cost_best = cost_\n",
        "            S.append(L-i)\n",
        "            S_best = S\n",
        "    return S_best, cost_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffYWtK1eLQRL",
        "outputId": "a7b791fb-7c54-4c1c-f7bb-f457bb923673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2, 1, 2, 2, 2, 1, 1, 1], 15)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "L = 12\n",
        "k = 8\n",
        "cost = [2,1,1,3] * 12\n",
        "f(L, 0, cost, k, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSgUxlfPLQRL"
      },
      "outputs": [],
      "source": [
        "#@jit(nopython=True)\n",
        "def pipe_dp(L, cost_e, cost_c, k, B):\n",
        "    # Generate all possible max length\n",
        "    possible = [0]\n",
        "    \n",
        "    for i in range(1, L+1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible.append(sum(cost_e[ptr:ptr+i]))\n",
        "            ptr += 1\n",
        "    \n",
        "    possible = sorted(list(set(possible)))\n",
        "    # print(possible)\n",
        "    # trace will be a 3D list\n",
        "    trace = []\n",
        "    for i in range(L):\n",
        "        outer = []\n",
        "        for j in range(k):\n",
        "            inner = []\n",
        "            for m in range(len(possible)):\n",
        "                inner.append(([],np.infty))\n",
        "            outer.append(inner)\n",
        "        trace.append(outer)\n",
        "    \n",
        "    # i: layer id, starting from 0\n",
        "    # j: number of cut (=GPU-1)\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(len(possible)):\n",
        "                if i+1 <= j: # invalid\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0: # base case: 0 cut\n",
        "                        cur_sum = sum(cost_e[:i+1])\n",
        "                        assert cur_sum in possible\n",
        "                        trace[i][j][m] = ([i+1], (B-1) * max(0, cur_sum - possible[m]))\n",
        "                    else:\n",
        "                        cost_best = np.infty\n",
        "                        S_best = []\n",
        "                        for cut in range(j-1, i):\n",
        "                            cur_sum = sum(cost_e[cut+1:i+1])\n",
        "                            assert cur_sum in possible\n",
        "                            S, cost_ = trace[cut][j-1][possible.index(max(cur_sum, possible[m]))]\n",
        "                            #print(S, cost_)\n",
        "                            cost_ += (B-1) * max(0, cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut][j-1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_ = copy.deepcopy(S)\n",
        "                                S_.append(i-cut)\n",
        "                                S_best = S_\n",
        "                        trace[i][j][m] = (S_best, cost_best)\n",
        "                        \n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            pass\n",
        "            #print(i, j, trace[i][j])\n",
        "    return trace[L-1][k-1][0]\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def pipe_dp_v2(L, cost_e, cost_c, k, B):\n",
        "    possible = np.zeros(L + 1, dtype=np.int64)\n",
        "\n",
        "    for i in range(1, L + 1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible[ptr + i - 1] = sum(cost_e[ptr:ptr + i])\n",
        "            ptr += 1\n",
        "\n",
        "    possible = np.unique(possible)\n",
        "\n",
        "    trace = np.empty((L, k, possible.shape[0]), dtype=np.object_)\n",
        "\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(possible.shape[0]):\n",
        "                if i + 1 <= j:\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0:\n",
        "                        cur_sum = sum(cost_e[:i + 1])\n",
        "                        trace[i, j, m] = (np.array([i + 1], dtype=np.int64), (B - 1) * max(0, cur_sum - possible[m]))\n",
        "                    else:\n",
        "                        cost_best = np.inf\n",
        "                        S_best = np.empty(0, dtype=np.int64)\n",
        "                        for cut in range(j - 1, i):\n",
        "                            cur_sum = sum(cost_e[cut + 1:i + 1])\n",
        "                            S, cost_ = trace[cut, j - 1, np.where(possible == max(cur_sum, possible[m]))[0][0]]\n",
        "                            cost_ += (B - 1) * max(0, cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut, j - 1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_ = S.copy()\n",
        "                                S_best = np.append(S_, i - cut)\n",
        "                        trace[i, j, m] = (S_best, cost_best)\n",
        "\n",
        "    return trace[L - 1, k - 1, 0]\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def pipe_dp_v3(L, cost_e, cost_c, k, B):\n",
        "    possible = np.zeros(L + 1, dtype=np.int64)\n",
        "\n",
        "    for i in range(1, L + 1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible[ptr + i - 1] = sum(cost_e[ptr:ptr + i])\n",
        "            ptr += 1\n",
        "\n",
        "    possible = np.unique(possible)\n",
        "\n",
        "    trace = np.empty((L, k, possible.shape[0]), dtype=np.object_)\n",
        "\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(possible.shape[0]):\n",
        "                if i + 1 <= j:\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0:\n",
        "                        cur_sum = sum(cost_e[:i + 1])\n",
        "                        trace[i, j, m] = (np.array([i + 1], dtype=np.int64), (B - 1) * max(0, cur_sum - possible[m]))\n",
        "                    else:\n",
        "                        cost_best = np.inf\n",
        "                        S_best = np.empty(0, dtype=np.int64)\n",
        "                        for cut in range(j - 1, i):\n",
        "                            cur_sum = sum(cost_e[cut + 1:i + 1])\n",
        "                            S, cost_ = trace[cut, j - 1, np.where(possible == max(cur_sum, possible[m]))[0][0]]\n",
        "                            cost_ += (B - 1) * max(0, cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut, j - 1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_ = S.copy()\n",
        "                                S_best = np.append(S_, i - cut)\n",
        "                        trace[i, j, m] = (S_best, cost_best)\n",
        "\n",
        "    return trace[L - 1, k - 1, 0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def pipe_dp_v4(L, cost_e, cost_c, k, B):\n",
        "    possible = np.zeros(L + 1, dtype=np.int64)\n",
        "\n",
        "    for i in range(1, L + 1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible[ptr + i - 1] = sum(cost_e[ptr:ptr + i])\n",
        "            ptr += 1\n",
        "\n",
        "    possible = np.unique(possible)\n",
        "\n",
        "    trace_S = np.empty((L, k, possible.shape[0]), dtype=np.int64)\n",
        "    trace_cost = np.full((L, k, possible.shape[0]), np.inf)\n",
        "\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(possible.shape[0]):\n",
        "                if i + 1 <= j:\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0:\n",
        "                        cur_sum = sum(cost_e[:i + 1])\n",
        "                        trace_S[i, j, m] = i + 1\n",
        "                        trace_cost[i, j, m] = (B - 1) * max(0, cur_sum - possible[m])\n",
        "                    else:\n",
        "                        cost_best = np.inf\n",
        "                        S_best = -1\n",
        "                        for cut in range(j - 1, i):\n",
        "                            cur_sum = sum(cost_e[cut + 1:i + 1])\n",
        "                            S, cost_ = trace_S[cut, j - 1, np.where(possible == max(cur_sum, possible[m]))[0][0]], trace_cost[cut, j - 1, np.where(possible == max(cur_sum, possible[m]))[0][0]]\n",
        "                            cost_ += (B - 1) * max(0, cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut, j - 1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_best = S\n",
        "                        trace_S[i, j, m] = S_best\n",
        "                        trace_cost[i, j, m] = cost_best\n",
        "\n",
        "    return trace_S[L - 1, k - 1], trace_cost[L - 1, k - 1]\n",
        "\n",
        "\n",
        "\n",
        "#@jit(nopython=True)\n",
        "def brute_force(L, cost_e, cost_c, k, B):\n",
        "    best_S = []\n",
        "    best_cost = np.infty\n",
        "    ptr_done = [0] * (k-1)\n",
        "    possible = list(itertools.combinations(range(L-1), k-1))\n",
        "    for p in possible:\n",
        "        p = list(p)\n",
        "        p.append(L-1)\n",
        "        lens = [sum(cost_e[:p[0]+1])]\n",
        "        s = [p[0]+1]\n",
        "        for i in range(len(p)-1):\n",
        "            lens.append(sum(cost_e[p[i]+1:p[i+1]+1]))\n",
        "            s.append(p[i+1]-p[i])     \n",
        "        max_l = max(lens)\n",
        "        cost = (B-1) * max_l\n",
        "        for i in range(k-1):\n",
        "            cost += cost_c[p[i]][i]\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_S = s\n",
        "    return best_S, best_cost\n",
        "\n",
        "#@jit(nopython=True)\n",
        "def uniform_split(L, cost_e, cost_c, k, B):\n",
        "    per_stage = L // k\n",
        "    \n",
        "    s = [per_stage] * (k-1)\n",
        "    s.append(L-sum(s))\n",
        "    p = [s[0]-1]\n",
        "    for i in range(1, k):\n",
        "        p.append(p[i-1] + s[i])\n",
        "    lens = [sum(cost_e[:p[0]+1])]\n",
        "    for i in range(len(s)-1):\n",
        "        lens.append(sum(cost_e[p[i]+1:p[i+1]+1]))\n",
        "    max_l = max(lens)\n",
        "    cost = (B-1) * max_l\n",
        "    for i in range(k-1):\n",
        "        cost += cost_c[p[i]][i]\n",
        "    return s, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBDBuHKRLQRM",
        "outputId": "61439acb-5da0-45b3-b480-55ec98fa16aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3, 1], 14.0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "L = 4\n",
        "k = 2\n",
        "cost_e = np.array([1, 3, 2, 5])\n",
        "#cost_e = [1,3,2,5]\n",
        "cost_c = np.ones((L-1, k-1)) * 2\n",
        "pipe_dp(L, cost_e, cost_c, k, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "def replace_nan(tensor, value=1e-6):\n",
        "    return torch.where(torch.isnan(tensor), torch.full_like(tensor, value), tensor)\n",
        "\n",
        "\n",
        "def softargmax2d(input, beta=100):\n",
        "    *_, h, w = input.shape\n",
        "    \n",
        "    input = input.reshape(*_, h * w)\n",
        "    input = replace_nan(input)  # Add this line\n",
        "    input = nn.functional.softmax(beta * input, dim=-1)\n",
        "\n",
        "    indices_c, indices_r = np.meshgrid(\n",
        "        np.linspace(0, 1, w),\n",
        "        np.linspace(0, 1, h),\n",
        "        indexing='xy'\n",
        "    )\n",
        "\n",
        "    indices_r = torch.tensor(np.reshape(indices_r, (-1, h * w)))\n",
        "    indices_c = torch.tensor(np.reshape(indices_c, (-1, h * w)))\n",
        "\n",
        "    result_r = torch.sum((h - 1) * input * indices_r, dim=-1)\n",
        "    result_c = torch.sum((w - 1) * input * indices_c, dim=-1)\n",
        "\n",
        "    result = torch.stack([result_r, result_c], dim=-1)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "MqyGXyMLkzjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "def pipe_dp_differentiable(L, cost_e, cost_c, k, B,position):\n",
        "    # Generate all possible max length\n",
        "    possible = [0]\n",
        "\n",
        "    for i in range(1, L + 1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible.append(sum(cost_e[ptr:ptr + i]))\n",
        "            ptr += 1\n",
        "\n",
        "    possible = sorted(list(set(possible)))\n",
        "\n",
        "    # trace will be a 3D list\n",
        "    trace = []\n",
        "    for i in range(L):\n",
        "        outer = []\n",
        "        for j in range(k):\n",
        "            inner = []\n",
        "            for m in range(len(possible)):\n",
        "                inner.append(([], torch.tensor(float('inf'))))\n",
        "            outer.append(inner)\n",
        "        trace.append(outer)\n",
        "\n",
        "    # i: layer id, starting from 0\n",
        "    # j: number of cut (=GPU-1)\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(len(possible)):\n",
        "                if i + 1 <= j:  # invalid\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0:  # base case: 0 cut\n",
        "                        cur_sum = sum(cost_e[:i + 1])\n",
        "                        trace[i][j][m] = ([i + 1], (B - 1) * max(0, cur_sum - possible[m]))\n",
        "                    else:\n",
        "                        cost_best = torch.tensor(float('inf'))\n",
        "                        S_best = []\n",
        "                        for cut in range(j - 1, i):\n",
        "                            cur_sum = sum(cost_e[cut + 1:i + 1])\n",
        "                            S, cost_ = trace[cut][j - 1][possible.index(max(cur_sum, possible[m]))]\n",
        "                            cost_ += (B - 1) * max(0, cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut][j - 1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_ = copy.deepcopy(S)\n",
        "                                S_.append(i - cut)\n",
        "                                S_best = S_\n",
        "                        trace[i][j][m] = (S_best, cost_best)\n",
        "\n",
        "    # Replace the non-differentiable control-flow statements with softargmax2d\n",
        "    \n",
        "    \n",
        "    cost_matrix = torch.zeros((L, k, len(possible)))\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(len(possible)):\n",
        "                cost_matrix[i, j, m] = trace[i][j][m][1]\n",
        "    softargmax_result = softargmax2d(cost_matrix)\n",
        "    softargmax_result=replace_nan(softargmax_result)\n",
        "    \n",
        "    '''\n",
        "    if cost_matrix is None:\n",
        "        cost_matrix = torch.zeros((L, k, len(possible)))\n",
        "        for i in range(L):\n",
        "            for j in range(k):\n",
        "                for m in range(len(possible)):\n",
        "                    cost_matrix[i, j, m] = trace[i][j][m][1]\n",
        "\n",
        "    softargmax_result = softargmax2d(cost_matrix)\n",
        "    softargmax_result = replace_nan(softargmax_result)\n",
        "    '''\n",
        "    \n",
        "    # Compute the loss using the softargmax_result\n",
        "    i = int(softargmax_result[0, 0].item())\n",
        "    j = int(softargmax_result[0, 1].item())\n",
        "    m = possible.index(max(sum(cost_e[i + 1:j + 1]), possible[j]))\n",
        "    #loss = trace[i][j][m][1]\n",
        "    loss = trace[i][j][m][1] + position[i, j, m]\n",
        "\n",
        "    return softargmax_result, loss,i,j\n",
        "\n",
        "\n",
        "def pipe_dp_differentiable_original(L, cost_e, cost_c, k, B):\n",
        "    cost_e = torch.tensor(cost_e, requires_grad=True, dtype=torch.float32)\n",
        "    cost_c = torch.tensor(cost_c, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "    # Generate all possible max length\n",
        "    possible = [0]\n",
        "    for i in range(1, L+1):\n",
        "        ptr = 0\n",
        "        while ptr + i <= L:\n",
        "            possible.append(torch.sum(cost_e[ptr:ptr+i]).item())\n",
        "            ptr += 1\n",
        "    possible = sorted(list(set(possible)))\n",
        "    possible = torch.tensor(possible, dtype=torch.float32)\n",
        "\n",
        "    trace = []\n",
        "    for i in range(L):\n",
        "        outer = []\n",
        "        for j in range(k):\n",
        "            inner = []\n",
        "            for m in range(len(possible)):\n",
        "                inner.append(([], torch.tensor(np.inf, dtype=torch.float32)))\n",
        "            outer.append(inner)\n",
        "        trace.append(outer)\n",
        "\n",
        "    for i in range(L):\n",
        "        for j in range(k):\n",
        "            for m in range(len(possible)):\n",
        "                if i+1 <= j: # invalid\n",
        "                    pass\n",
        "                else:\n",
        "                    if j == 0: # base case: 0 cut\n",
        "                        cur_sum = torch.sum(cost_e[:i+1])\n",
        "                        assert cur_sum.item() in possible\n",
        "                        trace[i][j][m] = ([i+1], (B-1) * torch.relu(cur_sum - possible[m]))\n",
        "                    else:\n",
        "                        cost_best = torch.tensor(np.inf, dtype=torch.float32)\n",
        "                        S_best = []\n",
        "                        for cut in range(j-1, i):\n",
        "                            cur_sum = torch.sum(cost_e[cut+1:i+1])\n",
        "                            assert cur_sum.item() in possible\n",
        "                            S, cost_ = trace[cut][j-1][torch.where(possible == torch.max(cur_sum, possible[m]))[0]]\n",
        "                            cost_ += (B-1) * torch.relu(cur_sum - possible[m])\n",
        "                            cost_ += cost_c[cut][j-1]\n",
        "                            if cost_ < cost_best:\n",
        "                                cost_best = cost_\n",
        "                                S_ = copy.deepcopy(S)\n",
        "                                S_.append(i-cut)\n",
        "                                S_best = S_\n",
        "                        trace[i][j][m] = (S_best, cost_best)\n",
        "\n",
        "    result_S, loss = trace[L-1][k-1][0]\n",
        "    return result_S, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "ycspvgOrbgDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Parameters\n",
        "L = 4\n",
        "k = 2\n",
        "cost_e = np.array([1, 3, 2, 5])\n",
        "cost_c = np.ones((L - 1, k - 1)) * 2\n",
        "B = 3\n",
        "\n",
        "# Convert to tensors\n",
        "cost_e_tensor = torch.tensor(cost_e, dtype=torch.float32)\n",
        "cost_c_tensor = torch.tensor(cost_c, dtype=torch.float32)\n",
        "\n",
        "# Generate all possible max length\n",
        "possible = [0]\n",
        "for i in range(1, L + 1):\n",
        "    ptr = 0\n",
        "    while ptr + i <= L:\n",
        "        possible.append(sum(cost_e[ptr:ptr + i]))\n",
        "        ptr += 1\n",
        "possible = sorted(list(set(possible)))\n",
        "\n",
        "# Initialize position tensor\n",
        "position = torch.zeros((L, k, len(possible)), requires_grad=True)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 5000\n",
        "lr = 1e-2\n",
        "optimizer = optim.SGD([position], lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Run the modified pipe_dp_differentiable function with the position tensor\n",
        "    result_S, loss, i, j = pipe_dp_differentiable(L, cost_e_tensor, cost_c_tensor, k, B, position=position)\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the position tensor using gradient descent\n",
        "    optimizer.step()\n",
        "\n",
        "    # Find the index of the maximum value in the result_S tensor along the possible dimension\n",
        "    max_idx = torch.argmax(result_S[i, j])\n",
        "\n",
        "    # Update position tensor with the maximum value at i, j indices\n",
        "    with torch.no_grad():\n",
        "        position[i, j, :] = 0\n",
        "        position[i, j, max_idx] = result_S[i, j, max_idx]\n",
        "\n",
        "    # Print the results\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch}: Loss = {loss.item()}, Final Result S: {result_S}', \"i=\", i, \"j=\", j)\n",
        "\n",
        "print(f'Final Result S: {result_S}', \"i=\", i, \"j=\", j)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "EMihGoIRlyC4",
        "outputId": "5f5abe0e-657e-4066-e762-c7005da54195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b5874afae49c>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_S\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi49SwGOLQRN"
      },
      "outputs": [],
      "source": [
        "test_list = [(12, 4), (24, 4), (24,8), (24, 12), (36, 8)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuFDg9SsLQRN",
        "outputId": "ecfc2e26-ba46-4d58-90cd-3117c7abad35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "homo dp L=12 k=4 is [3, 3, 3, 3], minimum cost 12.0. Took time 0.011282682418823242\n",
            "homo bf L=12 k=4 is [3, 3, 3, 3], minimum cost 12.0. Took time 0.001417398452758789\n",
            "homo us L=12 k=4 is [3, 3, 3, 3], minimum cost 12.0. Took time 3.0279159545898438e-05\n",
            "homo dp L=24 k=4 is [6, 6, 6, 6], minimum cost 18.0. Took time 0.10501551628112793\n",
            "homo bf L=24 k=4 is [6, 6, 6, 6], minimum cost 18.0. Took time 0.021759510040283203\n",
            "homo us L=24 k=4 is [6, 6, 6, 6], minimum cost 18.0. Took time 2.288818359375e-05\n",
            "homo dp L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.19997739791870117\n",
            "homo bf L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 3.9657673835754395\n",
            "homo us L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 5.221366882324219e-05\n",
            "homo dp L=24 k=12 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 26.0. Took time 0.29858970642089844\n",
            "homo bf L=24 k=12 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 26.0. Took time 30.561776638031006\n",
            "homo us L=24 k=12 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 26.0. Took time 6.246566772460938e-05\n",
            "homo dp L=36 k=8 is [1, 5, 5, 5, 5, 5, 5, 5], minimum cost 24.0. Took time 0.7829232215881348\n",
            "homo bf L=36 k=8 is [1, 5, 5, 5, 5, 5, 5, 5], minimum cost 24.0. Took time 115.16702842712402\n",
            "homo us L=36 k=8 is [4, 4, 4, 4, 4, 4, 4, 8], minimum cost 30.0. Took time 5.984306335449219e-05\n"
          ]
        }
      ],
      "source": [
        "# homogeneous test\n",
        "for L, k in test_list:\n",
        "    cost_e = np.ones(L)\n",
        "    cost_c = np.ones((L-1, k-1)) * 2\n",
        "    time_s = time.time()\n",
        "    res = pipe_dp(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"homo dp L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")\n",
        "    time_s = time.time()\n",
        "    res = brute_force(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"homo bf L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")\n",
        "    time_s = time.time()\n",
        "    res = uniform_split(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"homo us L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnA1NWzALQRO",
        "outputId": "bb4eab19-8f80-4b6c-e3cd-cbd35e489484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hete dp L=12 k=4 is [3, 3, 3, 3], minimum cost 70. Took time 0.05019879341125488\n",
            "hete bf L=12 k=4 is [3, 3, 3, 3], minimum cost 70. Took time 0.0013661384582519531\n",
            "hete us L=12 k=4 is [3, 3, 3, 3], minimum cost 70. Took time 1.9788742065429688e-05\n",
            "hete dp L=24 k=4 is [6, 6, 6, 6], minimum cost 115. Took time 0.7052814960479736\n",
            "hete bf L=24 k=4 is [6, 6, 6, 6], minimum cost 115. Took time 0.016564130783081055\n",
            "hete us L=24 k=4 is [6, 6, 6, 6], minimum cost 115. Took time 2.193450927734375e-05\n",
            "hete dp L=24 k=8 is [3, 4, 3, 2, 3, 3, 3, 3], minimum cost 95. Took time 1.4543581008911133\n",
            "hete bf L=24 k=8 is [3, 4, 3, 2, 3, 3, 3, 3], minimum cost 95. Took time 3.7701711654663086\n",
            "hete us L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 97. Took time 4.601478576660156e-05\n",
            "hete dp L=24 k=12 is [1, 2, 3, 2, 1, 2, 1, 3, 2, 3, 2, 2], minimum cost 104. Took time 1.8105647563934326\n",
            "hete bf L=24 k=12 is [1, 2, 3, 2, 1, 2, 1, 3, 2, 3, 2, 2], minimum cost 104. Took time 28.778594970703125\n",
            "hete us L=24 k=12 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 109. Took time 5.888938903808594e-05\n",
            "hete dp L=36 k=8 is [3, 5, 4, 5, 5, 6, 4, 4], minimum cost 117. Took time 7.158376693725586\n",
            "hete bf L=36 k=8 is [3, 5, 4, 5, 5, 6, 4, 4], minimum cost 117. Took time 108.56706666946411\n",
            "hete us L=36 k=8 is [4, 4, 4, 4, 4, 4, 4, 8], minimum cost 170. Took time 7.009506225585938e-05\n"
          ]
        }
      ],
      "source": [
        "# hetergeneous test\n",
        "for L, k in test_list:\n",
        "    cost_e = np.random.randint(low=5,high=10,size=L)\n",
        "    cost_c = np.random.randint(low=5,high=10,size=(L-1,k-1))\n",
        "    time_s = time.time()\n",
        "    res = pipe_dp(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"hete dp L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")\n",
        "    time_s = time.time()\n",
        "    res = brute_force(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"hete bf L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")\n",
        "    time_s = time.time()\n",
        "    res = uniform_split(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"hete us L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCxoBCT3LQRO",
        "outputId": "f6ceffda-cc37-4a1e-e300-57ae8c050d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hete dp L=12 k=4 is [3, 3, 3, 3], minimum cost 68. Took time 0.04140925407409668\n",
            "hete us L=12 k=4 is [3, 3, 3, 3], minimum cost 68. Took time 2.3603439331054688e-05\n",
            "hete dp L=24 k=12 is [2, 2, 2, 1, 2, 2, 2, 2, 3, 3, 1, 2], minimum cost 97. Took time 1.7796664237976074\n",
            "hete us L=24 k=12 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 107. Took time 4.839897155761719e-05\n",
            "hete dp L=36 k=8 is [4, 6, 4, 4, 4, 5, 5, 4], minimum cost 115. Took time 7.414276838302612\n",
            "hete us L=36 k=8 is [4, 4, 4, 4, 4, 4, 4, 8], minimum cost 161. Took time 4.6253204345703125e-05\n",
            "hete dp L=36 k=12 is [1, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3], minimum cost 121. Took time 9.608815431594849\n",
            "hete us L=36 k=12 is [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], minimum cost 126. Took time 5.3882598876953125e-05\n",
            "hete dp L=48 k=12 is [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], minimum cost 129. Took time 30.9409818649292\n",
            "hete us L=48 k=12 is [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], minimum cost 129. Took time 5.507469177246094e-05\n",
            "hete dp L=48 k=24 is [3, 3, 3, 2, 3, 4, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 4, 3], minimum cost 176. Took time 55.34712266921997\n",
            "hete us L=48 k=24 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], minimum cost 197. Took time 7.867813110351562e-05\n",
            "hete dp L=64 k=12 is [5, 6, 2, 6, 6, 6, 6, 5, 4, 6, 6, 6], minimum cost 148. Took time 97.49400663375854\n",
            "hete us L=64 k=12 is [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9], minimum cost 217. Took time 7.009506225585938e-05\n",
            "hete dp L=64 k=16 is [5, 4, 2, 2, 5, 4, 5, 2, 4, 4, 4, 5, 5, 4, 5, 4], minimum cost 157. Took time 130.8333797454834\n",
            "hete us L=64 k=16 is [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], minimum cost 180. Took time 6.508827209472656e-05\n",
            "hete dp L=128 k=32 is [4, 4, 4, 4, 5, 3, 5, 2, 4, 4, 5, 4, 5, 5, 5, 4, 1, 4, 5, 6, 5, 5, 6, 4, 4, 3, 1, 5, 4, 5, 2, 1], minimum cost 242. Took time 3688.281402349472\n",
            "hete us L=128 k=32 is [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], minimum cost 264. Took time 0.00010204315185546875\n",
            "hete dp L=128 k=12 is [10, 10, 10, 12, 11, 10, 11, 9, 11, 11, 12, 11], minimum cost 227. Took time 1504.3015270233154\n",
            "hete us L=128 k=12 is [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 18], minimum cost 319. Took time 7.486343383789062e-05\n",
            "hete dp L=128 k=50 is [1, 1, 3, 5, 4, 1, 1, 4, 1, 3, 1, 4, 4, 3, 1, 1, 4, 3, 1, 3, 2, 3, 1, 4, 5, 3, 2, 3, 3, 1, 1, 1, 3, 3, 3, 2, 4, 4, 4, 1, 2, 2, 4, 1, 2, 3, 1, 4, 5, 2], minimum cost 328. Took time 5430.115435838699\n",
            "hete us L=128 k=50 is [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 30], minimum cost 782. Took time 0.00014972686767578125\n"
          ]
        }
      ],
      "source": [
        "test_list_large = [(12, 4), (24, 12), (36, 8), (36, 12), (48,12), (48, 24), (64, 12), (64, 16), (128, 32), (128, 12), (128, 50)]\n",
        "for L, k in test_list_large:\n",
        "    cost_e = np.random.randint(low=5,high=10,size=L)\n",
        "    cost_c = np.random.randint(low=5,high=10,size=(L-1,k-1))\n",
        "    time_s = time.time()\n",
        "    res = pipe_dp(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"hete dp L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")\n",
        "    time_s = time.time()\n",
        "    res = uniform_split(L, cost_e, cost_c, k, 3)\n",
        "    print(f\"hete us L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time.time() - time_s}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E71Et0BLQRP",
        "outputId": "be02c1e5-8523-4446-809a-3eca7421d652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "homo dp L=16 k=8 is [2, 2, 2, 2, 2, 2, 2, 2], minimum cost 18.0. Took time 0.05113816261291504\n",
            "homo bf L=16 k=8 is [2, 2, 2, 2, 2, 2, 2, 2], minimum cost 18.0. Took time 0.09984564781188965\n",
            "homo dp L=17 k=8 is [1, 1, 1, 2, 3, 3, 3, 3], minimum cost 20.0. Took time 0.06062602996826172\n",
            "homo bf L=17 k=8 is [1, 1, 1, 2, 3, 3, 3, 3], minimum cost 20.0. Took time 0.17916107177734375\n",
            "homo dp L=18 k=8 is [1, 1, 1, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.07352972030639648\n",
            "homo bf L=18 k=8 is [1, 1, 1, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.30548691749572754\n",
            "homo dp L=19 k=8 is [1, 1, 2, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.08911275863647461\n",
            "homo bf L=19 k=8 is [1, 1, 2, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.5053002834320068\n",
            "homo dp L=20 k=8 is [1, 1, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.10455751419067383\n",
            "homo bf L=20 k=8 is [1, 1, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.7985565662384033\n",
            "homo dp L=21 k=8 is [1, 2, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.1241300106048584\n",
            "homo bf L=21 k=8 is [1, 2, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 1.2353487014770508\n",
            "homo dp L=22 k=8 is [1, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.14544916152954102\n",
            "homo bf L=22 k=8 is [1, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 1.8621459007263184\n",
            "homo dp L=23 k=8 is [2, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.16782760620117188\n",
            "homo bf L=23 k=8 is [2, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 2.7482845783233643\n",
            "homo dp L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 0.1974644660949707\n",
            "homo bf L=24 k=8 is [3, 3, 3, 3, 3, 3, 3, 3], minimum cost 20.0. Took time 3.9475131034851074\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "test_list = [(16,8), (17, 8), (18,8), (19,8), (20, 8), (21,8), (22,8), (23, 8),(24,8)]\n",
        "dp_time = []\n",
        "bf_time = []\n",
        "\n",
        "# homogeneous test\n",
        "for L, k in test_list:\n",
        "    cost_e = np.ones(L)\n",
        "    cost_c = np.ones((L-1, k-1)) * 2\n",
        "    time_s = time.time()\n",
        "    res = pipe_dp(L, cost_e, cost_c, k, 3)\n",
        "    time_elapsed = time.time() - time_s\n",
        "    dp_time.append(time_elapsed)\n",
        "    print(f\"homo dp L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time_elapsed}\")\n",
        "    time_s = time.time()\n",
        "    res = brute_force(L, cost_e, cost_c, k, 3)\n",
        "    time_elapsed = time.time() - time_s\n",
        "    bf_time.append(time_elapsed)\n",
        "    print(f\"homo bf L={L} k={k} is {res[0]}, minimum cost {res[1]}. Took time {time_elapsed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "69nTOIEBLQRP",
        "outputId": "3b7f75de-e739-4eb9-ccc5-f5711f0b1b63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f35ee31add0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot([16,17, 18, 19, 20, 21, 22, 23, 24], dp_time, label=\"ours\")\n",
        "plt.plot([16,17, 18, 19, 20, 21, 22, 23, 24], bf_time, label=\"brute force\")\n",
        "plt.xlabel(\"number of layers\")\n",
        "plt.ylabel(\"runtime\")\n",
        "plt.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7_vTnJRLQRP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}